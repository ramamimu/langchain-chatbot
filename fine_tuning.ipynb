{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ram/Codes/college/chatbot/local-langchain/venv/lib/python3.8/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values in dataset:\n",
      " question    0\n",
      "context     0\n",
      "dtype: int64\n",
      "Dataset after cleaning:\n",
      "                                             question  \\\n",
      "0          MyITS saya bermasalah, bisa lapor kemana?   \n",
      "1  Saya ingin mengurus '.....' siapa tendik yang ...   \n",
      "2                         Cara mendapatkan transkrip   \n",
      "3  Cara mendapatkan surat keterangan aktif mahasiswa   \n",
      "4  Bagaimana mendapatkan translasi ke dalam Bahas...   \n",
      "\n",
      "                                             context  \n",
      "0  Silakan ajukan tiket ke DPTSI di https://servi...  \n",
      "1  Temui tendik sesuai bidangnya:\\n\\n- Persuratan...  \n",
      "2  Buka SIM Akademik, masuk ke menu LAPORAN -> TR...  \n",
      "3  Buka SIM Akademik, masuk ke menu SURAT MAHASIS...  \n",
      "4  Translasi dilayani oleh BURB.\\nKeluhan/permint...  \n",
      "Number of validation examples: 71\n",
      "Sample validation data:                                               question  \\\n",
      "296  Apa kriteria pekerjaan yang bisa dikerjakan ol...   \n",
      "81                  Jelaskan tentang program Exchange?   \n",
      "77   Apa yang harus dilakukan peserta selama sesi t...   \n",
      "208  Apa yang harus dipenuhi oleh mahasiswa luar IT...   \n",
      "318                Dimana layanan LHKPN dapat diakses?   \n",
      "\n",
      "                                               context  \n",
      "296  Kriteria Pekerjaan yang Bisa Dikerjakan oleh T...  \n",
      "81   The opportunities to study abroad with ITS par...  \n",
      "77   Selama sesi diskusi dan tanya jawab, peserta s...  \n",
      "208  Persyaratan untuk mahasiswa luar ITS adalah me...  \n",
      "318  Layanan dapat diakses pada tautan: elhkpn.kpk....  \n",
      "Example 1: Question - Apa kriteria pekerjaan yang bisa dikerjakan oleh tenaga magang mahasiswa?, Context - Kriteria Pekerjaan yang Bisa Dikerjakan oleh Tenaga Magang Mahasiswa:\n",
      "1. Bukan merupakan pekerjaan utama (core)\n",
      "2. Bukan merupakan pekerjaan rutin\n",
      "3. Merupakan pekerjaan pengembangan/proyek\n",
      "4. Durasi magang maksimal 6 bulan\n",
      "Example 2: Question - Jelaskan tentang program Exchange?, Context - The opportunities to study abroad with ITS partner university, with the duration depending on each university (1-2 semesters) and have to make credit transfer\n",
      "Example 3: Question - Apa yang harus dilakukan peserta selama sesi tanya jawab dalam sidang proposal tesis?, Context - Selama sesi diskusi dan tanya jawab, peserta sidang proposal tesis diharapkan dapat mencatat masukan dan koreksi dari dosen penguji.\n",
      "Example 4: Question - Apa yang harus dipenuhi oleh mahasiswa luar ITS dalam melaksanakan MBKM di ITS?, Context - Persyaratan untuk mahasiswa luar ITS adalah memiliki akreditasi Prodi asal mahasiswa minimal sama dengan Prodi di ITS. Jumlah mahasiswa dari luar ITS dibatasi maksimal 10% dari daya tampung Prodi di ITS.\n",
      "Example 5: Question - Dimana layanan LHKPN dapat diakses?, Context - Layanan dapat diakses pada tautan: elhkpn.kpk.go.id\n",
      "First few training examples: [<sentence_transformers.readers.InputExample.InputExample object at 0x7f9313cb64f0>, <sentence_transformers.readers.InputExample.InputExample object at 0x7f9313d02e50>, <sentence_transformers.readers.InputExample.InputExample object at 0x7f9313cb65e0>, <sentence_transformers.readers.InputExample.InputExample object at 0x7f9313cb6460>, <sentence_transformers.readers.InputExample.InputExample object at 0x7f9313cb66a0>]\n",
      "First few validation examples: [<sentence_transformers.readers.InputExample.InputExample object at 0x7f9313c74040>, <sentence_transformers.readers.InputExample.InputExample object at 0x7f9428d08c40>, <sentence_transformers.readers.InputExample.InputExample object at 0x7f9313c74160>, <sentence_transformers.readers.InputExample.InputExample object at 0x7f9313c74100>, <sentence_transformers.readers.InputExample.InputExample object at 0x7f9313c74880>]\n",
      "Sample embeddings: [[ 3.91161591e-02 -5.02011813e-02 -4.58370708e-02 -6.17108755e-02\n",
      "   4.43586819e-02 -4.02299799e-02  1.80259682e-04  2.93737324e-03\n",
      "   5.20469099e-02  2.57273372e-02  7.34769553e-02  2.66735186e-03\n",
      "   6.46811500e-02 -2.80206650e-02 -3.18849683e-02  4.67182584e-02\n",
      "   6.30171522e-02 -6.77656457e-02 -1.87818762e-02 -4.45792265e-02\n",
      "   3.28683369e-02  2.47354596e-03 -3.77273522e-02  6.88401908e-02\n",
      "   3.80996242e-02  3.81029584e-02 -5.69080599e-02  3.77535708e-02\n",
      "   2.30360776e-02 -6.34261072e-02 -5.61309271e-02 -4.82164174e-02\n",
      "   4.61888053e-02 -5.40262572e-02  4.58056629e-02  2.38414910e-02\n",
      "  -7.26896971e-02 -9.67203546e-03  6.49136528e-02 -2.80329157e-02\n",
      "  -8.17957371e-02 -4.52795019e-03  2.53771860e-02  9.44345072e-02\n",
      "   6.00171797e-02  8.37175399e-02 -1.19530158e-02  7.90043846e-02\n",
      "   1.31594553e-03 -3.34680527e-02 -5.01634367e-02  7.47823715e-02\n",
      "  -8.19071010e-03  6.35935366e-03  5.89255476e-03 -5.47124967e-02\n",
      "  -2.15942971e-02 -4.32889573e-02 -5.04744984e-02  5.55245019e-02\n",
      "   3.37918513e-02 -5.59799746e-02 -2.61386577e-03  3.31822261e-02\n",
      "   5.53418174e-02  7.50579610e-02  4.44767205e-03  4.38620038e-02\n",
      "  -6.24160618e-02 -4.44597341e-02 -4.39686887e-02  5.71000390e-03\n",
      "   3.60880010e-02 -7.77576566e-02 -3.51144411e-02  4.51888740e-02\n",
      "   6.08787984e-02 -3.22730839e-02  2.85982192e-02 -3.31439227e-02\n",
      "  -5.54078333e-02 -9.49641503e-03 -5.31066731e-02  3.00586503e-02\n",
      "  -8.80990699e-02  6.20550662e-02  2.58843414e-02 -4.24859859e-02\n",
      "   6.71945736e-02 -8.78831372e-02  3.34029086e-02  4.83317040e-02\n",
      "  -4.90782410e-02 -6.84367865e-02 -7.59544596e-02 -7.37819895e-02\n",
      "  -5.69152832e-02  3.54814623e-03  2.06975993e-02 -4.75446228e-03\n",
      "   1.78994536e-02 -5.36858477e-02  4.54264656e-02 -1.46832168e-01\n",
      "  -6.33126944e-02  4.52387445e-02  2.18550786e-02 -6.71495721e-02\n",
      "   5.85880205e-02 -5.59617132e-02 -2.94375047e-02  9.51411277e-02\n",
      "   6.40937239e-02  7.21706077e-02 -3.73885706e-02 -3.15490887e-02\n",
      "  -3.30448709e-02 -4.38137650e-02  5.73908836e-02 -8.35132673e-02\n",
      "   6.31176382e-02  6.26899884e-04 -3.29010487e-02 -6.34943321e-02\n",
      "  -4.55198176e-02 -3.81391332e-03  6.63011968e-02  1.13281533e-02\n",
      "  -6.68186601e-03  1.89472027e-02  7.34278839e-03  4.16270159e-02\n",
      "   4.03705314e-02  6.74440414e-02  4.85300384e-02  7.55474716e-02\n",
      "  -6.38600066e-02  6.68535084e-02 -1.94495525e-02  1.10188536e-02\n",
      "  -5.25341965e-02  1.51265170e-02 -5.17724678e-02  3.51351574e-02\n",
      "   4.90773059e-02  6.09189235e-02  6.00163154e-02 -7.82415941e-02\n",
      "   7.33678490e-02 -6.20453469e-02  5.13100736e-02 -1.79322530e-02\n",
      "   4.65881340e-02  3.45624387e-02  4.75161262e-02 -1.71339475e-02\n",
      "  -5.32454513e-02 -6.32264838e-02  8.46852586e-02  7.83203915e-02\n",
      "  -1.07181361e-02 -4.92077880e-02 -8.10419023e-02 -5.44699356e-02\n",
      "  -1.88301411e-02 -1.54975671e-02  4.72641364e-02  3.86867374e-02\n",
      "  -5.39331399e-02 -5.70024103e-02 -4.74990867e-02  5.47855347e-02\n",
      "  -5.32451048e-02  3.53954807e-02 -2.13244162e-03  7.10649192e-02\n",
      "  -2.37070024e-02  8.98206234e-02  6.62881210e-02  3.05352155e-02\n",
      "   7.58566894e-03 -3.20050009e-02  2.77072880e-02 -4.79736365e-02\n",
      "  -3.06599122e-02 -1.04211234e-01 -3.79249863e-02  5.37012704e-02\n",
      "   6.52644560e-02 -1.60184186e-02 -2.22075190e-02  6.26003742e-02\n",
      "  -8.97529908e-03 -9.07767266e-02 -7.57214800e-02  2.36527920e-02\n",
      "  -2.84502488e-02  2.17100699e-02  5.06509356e-02  3.72438692e-02\n",
      "  -7.11494533e-04 -2.51536295e-02  3.51974182e-02  1.29949115e-02\n",
      "   2.36906502e-02  2.55187508e-02 -5.45628071e-02  3.16482037e-02\n",
      "  -7.90828019e-02  7.66317174e-02  3.20911221e-02 -1.01242987e-02\n",
      "  -4.15690467e-02  4.71940711e-02  6.99929878e-05  1.06780268e-02\n",
      "  -3.23941582e-03  5.44751845e-02 -1.87156349e-02 -1.85168236e-02\n",
      "   4.85832095e-02 -1.84362009e-02  5.96012361e-02 -7.68099874e-02\n",
      "  -6.19461872e-02 -8.66546948e-03  6.13214858e-02 -5.14099933e-02\n",
      "  -3.39077078e-02  8.96898098e-03 -2.56099459e-02 -6.29543066e-02\n",
      "  -9.56248045e-02 -7.74172544e-02  1.35703315e-03 -9.79679003e-02\n",
      "   1.74051840e-02  4.78824116e-02  9.44967568e-02 -9.05120522e-02\n",
      "  -9.49291214e-02 -2.48359442e-02  2.48961635e-02 -3.38558182e-02\n",
      "   9.64550972e-02 -5.03858142e-02 -4.40440103e-02  3.77539881e-02\n",
      "  -3.03242132e-02  7.04648718e-02  8.83444920e-02 -9.94721428e-02\n",
      "  -3.06145772e-02 -3.50104235e-02 -5.10665588e-02  6.00774772e-02\n",
      "   6.00009933e-02  3.59251797e-02 -6.08506203e-02  2.49627642e-02\n",
      "  -8.74955149e-04  1.32360579e-02  8.98826197e-02  6.77876454e-03\n",
      "   5.76400338e-03  1.63459629e-02 -6.13862388e-02  6.01449537e-05\n",
      "  -5.39362803e-02  1.35922814e-02 -4.43572216e-02  6.80964142e-02\n",
      "   1.09167479e-01 -1.39828427e-02 -1.58954214e-03 -5.14816567e-02\n",
      "  -1.43925007e-02  1.31057411e-01 -5.67289023e-03 -4.91372421e-02\n",
      "   6.77160472e-02  2.17497349e-02  3.89359668e-02  9.41475481e-02\n",
      "   3.94670442e-02 -1.98360160e-02 -7.55422574e-04  4.82828841e-02\n",
      "  -6.70946836e-02 -1.32608768e-02 -1.83262881e-02 -7.19855577e-02\n",
      "   7.42522180e-02 -8.01885128e-02  1.07807882e-01  4.43536341e-02\n",
      "  -9.36663896e-03  6.01106472e-02 -4.19218130e-02  2.99709477e-02\n",
      "  -4.68171109e-03 -3.17181274e-02  4.35781851e-02  6.21287897e-02\n",
      "  -1.20023802e-01  5.30349202e-02 -5.74857928e-04 -1.26455221e-02\n",
      "   2.13316102e-02  6.21339567e-02  7.57341757e-02  9.78858173e-02\n",
      "  -4.21844386e-02 -3.63502391e-02 -1.14966873e-02  2.08246559e-02\n",
      "  -1.33264752e-03  2.17060912e-02 -9.15000588e-02 -3.34701762e-02\n",
      "  -5.28723635e-02 -3.41672986e-03 -1.13114659e-02 -3.93859670e-02\n",
      "   4.73563485e-02  2.06408538e-02 -2.60289721e-02 -2.84431819e-02\n",
      "   1.87248066e-02 -2.35784776e-03  2.62339022e-02 -5.48329130e-02\n",
      "  -5.49074598e-02  6.27044067e-02 -4.09292467e-02 -7.62736127e-02\n",
      "   3.74363153e-03  1.66656077e-02 -4.21236232e-02 -2.04270110e-02\n",
      "   2.12627780e-02  7.89485592e-03 -2.29118280e-02  4.80398573e-02\n",
      "   2.35928921e-03 -2.04835385e-02 -4.48208069e-03 -7.20079169e-02\n",
      "  -2.69461256e-02  6.00468256e-02  1.56329498e-02 -7.65541866e-02\n",
      "   1.32336672e-02  4.59579602e-02 -1.12050146e-01  7.42270350e-02\n",
      "  -7.55912289e-02 -4.18470576e-02  3.57114635e-02  2.05158852e-02\n",
      "  -1.12312756e-01 -4.37237732e-02  2.47162990e-02  1.47264302e-02\n",
      "  -1.88677572e-02  4.67834882e-02 -2.83392612e-02 -1.46213649e-02\n",
      "   2.22970769e-02 -2.82019526e-02  5.99112883e-02  6.44821450e-02\n",
      "  -5.49893863e-02 -2.07638964e-02 -6.84551848e-03 -1.41899865e-02\n",
      "  -6.12650067e-02  5.65349013e-02 -2.28153169e-02 -3.48960087e-02\n",
      "   6.70570582e-02  8.52251202e-02 -3.09874192e-02  6.67733178e-02]\n",
      " [ 2.48362962e-02 -2.84856819e-02 -3.43256891e-02 -9.45122093e-02\n",
      "   5.82435876e-02 -6.41487539e-02  1.44656440e-02 -1.39302313e-02\n",
      "   4.26585749e-02  2.22018212e-02  6.92612305e-02  3.68014947e-02\n",
      "   6.65395111e-02 -2.71551013e-02 -2.88980957e-02  4.81446460e-02\n",
      "   6.24383017e-02 -6.43493161e-02 -5.79000771e-05 -2.70952359e-02\n",
      "   1.51348002e-02 -2.27500461e-02 -3.43496241e-02  5.01623340e-02\n",
      "   2.77054273e-02  1.65396314e-02 -1.48953693e-02  2.35373508e-02\n",
      "   5.60957491e-02 -4.53114845e-02 -5.09536788e-02 -2.83266772e-02\n",
      "   4.57635224e-02 -3.14388499e-02  2.99784634e-02  3.67692066e-03\n",
      "  -7.99339935e-02 -4.70022224e-02  5.23244925e-02 -3.93604487e-02\n",
      "  -3.09951641e-02  3.20196413e-02  4.16386984e-02  9.70000923e-02\n",
      "   4.30654027e-02  1.06082171e-01 -3.80416885e-02  6.47981316e-02\n",
      "   1.03615597e-02 -3.81487273e-02 -2.34096311e-02  7.64458030e-02\n",
      "   2.60292888e-02 -3.02586015e-02 -1.01788538e-02 -6.77637085e-02\n",
      "  -3.99090499e-02 -7.66194016e-02 -6.89828694e-02  1.85292661e-02\n",
      "   3.19670103e-02 -5.78683876e-02  4.10671812e-03  1.76920369e-02\n",
      "   6.72054961e-02  4.22793105e-02  7.12447148e-03  6.89408779e-02\n",
      "  -7.29468688e-02 -6.93796203e-02 -4.48352434e-02  3.31886038e-02\n",
      "   2.14699525e-02 -8.07074234e-02 -7.73440814e-03  6.96610138e-02\n",
      "   4.58786674e-02 -3.36647965e-02  1.93154812e-02 -4.17862758e-02\n",
      "  -6.00152910e-02 -4.45820577e-02 -4.29702438e-02  2.44490393e-02\n",
      "  -8.07047710e-02  7.05773979e-02  2.77905855e-02 -7.35233128e-02\n",
      "   7.15077892e-02 -7.98318088e-02  4.74362932e-02  2.23997906e-02\n",
      "  -4.33770269e-02 -8.61121416e-02 -6.23038970e-02 -9.45576429e-02\n",
      "  -2.95706056e-02  2.13216655e-02  2.77104881e-02 -1.13262255e-02\n",
      "   4.17902432e-02 -7.96575844e-02  1.93436239e-02 -1.43747196e-01\n",
      "  -2.07272684e-03  2.55721267e-02  4.24648412e-02 -7.48857632e-02\n",
      "   7.47957453e-02 -4.35835235e-02 -1.37206186e-02  5.89937903e-02\n",
      "   9.13578048e-02  4.50523831e-02 -6.31960556e-02 -2.26135403e-02\n",
      "  -2.81894971e-02 -7.59587362e-02  3.85219604e-02 -5.00825346e-02\n",
      "   4.32052687e-02  3.35793942e-02 -3.88533324e-02 -7.38568828e-02\n",
      "  -3.97651829e-02  2.01517250e-03  7.63807818e-02  2.23375745e-02\n",
      "   5.29191382e-02 -2.85571703e-04  5.55781536e-02  4.39177416e-02\n",
      "  -1.16616543e-02  6.46250397e-02  3.93987559e-02  6.82567731e-02\n",
      "  -5.39249368e-02  6.51700646e-02 -1.04119107e-02 -5.02201775e-03\n",
      "  -6.25712648e-02  2.80557685e-02 -8.49262029e-02  4.93779741e-02\n",
      "   7.26144835e-02  4.34908047e-02  8.12922120e-02 -4.54284847e-02\n",
      "   9.10365134e-02 -8.75467360e-02  4.90236729e-02 -2.19644587e-02\n",
      "   1.58868451e-02  4.62009050e-02  5.66857196e-02 -1.94793921e-02\n",
      "  -8.39791223e-02 -9.01477039e-02  7.91060627e-02  6.48740083e-02\n",
      "  -3.36447060e-02 -9.07581523e-02 -7.08022192e-02 -5.08488528e-02\n",
      "   5.33098180e-04 -1.45370495e-02  2.49637738e-02  3.84197347e-02\n",
      "  -4.02033329e-02 -2.50318609e-02 -4.11145203e-02  5.00352755e-02\n",
      "  -3.30798924e-02  3.53667699e-02 -2.43432075e-02  6.31295294e-02\n",
      "  -3.49998809e-02  5.97953387e-02  7.20688701e-02  2.64648907e-02\n",
      "  -5.14191994e-03 -4.59008478e-02  9.62378178e-03 -2.41828561e-02\n",
      "  -4.22232114e-02 -7.62391314e-02 -3.32747102e-02  3.61723639e-02\n",
      "   1.94198415e-02  2.56850128e-03 -2.09948272e-02  5.31180836e-02\n",
      "   2.21047793e-02 -9.51868445e-02 -8.48898143e-02 -2.17670873e-02\n",
      "  -4.80177701e-02  2.11609732e-02  5.13220280e-02  2.80775875e-02\n",
      "  -1.49060898e-02  9.05216765e-03  3.01044863e-02  3.25458907e-02\n",
      "  -2.18705554e-03  4.57545891e-02 -1.08181708e-01  3.18510011e-02\n",
      "  -5.62404655e-02  5.57875186e-02  4.56183441e-02 -1.30731212e-02\n",
      "  -4.93121892e-02  6.22698553e-02  3.55263386e-04  1.60053410e-02\n",
      "  -1.84040461e-02  4.69304137e-02 -3.20586637e-02  6.89712819e-03\n",
      "   5.15409522e-02 -5.21428091e-03  3.91790681e-02 -5.94344288e-02\n",
      "  -2.87115593e-02  2.40523797e-02  6.35602698e-02 -5.56908585e-02\n",
      "  -2.39514504e-02  2.22721882e-02 -2.16814782e-02 -9.52076390e-02\n",
      "  -6.48848563e-02 -6.64214864e-02  6.70273602e-03 -8.31372514e-02\n",
      "   1.37579758e-02  6.03350028e-02  8.30528885e-02 -7.85780475e-02\n",
      "  -4.64854799e-02 -2.36695409e-02  3.43455188e-02 -1.91054121e-02\n",
      "   8.50260034e-02 -2.83002984e-02 -6.69908002e-02  3.91142368e-02\n",
      "  -9.43130255e-03  5.09797074e-02  4.70920242e-02 -8.89836326e-02\n",
      "  -6.49212152e-02 -3.13294381e-02 -3.19324285e-02  5.95535040e-02\n",
      "   6.07583448e-02  1.51530690e-02 -3.67518775e-02  3.59287336e-02\n",
      "   2.78270035e-03 -2.77399383e-02  1.22720413e-01  8.18637025e-04\n",
      "   4.29735146e-02  2.49897223e-02 -7.11717159e-02 -2.34885737e-02\n",
      "  -5.61724268e-02 -2.17438303e-02 -2.87445355e-02  3.77519652e-02\n",
      "   6.89084902e-02 -2.89516114e-02  4.82563511e-04 -2.15037093e-02\n",
      "   1.85525846e-02  1.10681243e-01 -1.75091177e-02 -5.10119386e-02\n",
      "   4.49160114e-02  2.04538200e-02  4.53141555e-02  7.14982152e-02\n",
      "   6.28226176e-02 -2.21595336e-02 -2.48647295e-03  7.24441856e-02\n",
      "  -6.68215603e-02 -4.06568311e-02 -2.65807509e-02 -4.73848693e-02\n",
      "   5.97280040e-02 -7.62793422e-02  1.09758087e-01  3.73676382e-02\n",
      "  -9.16529540e-03  8.64717886e-02 -4.43590358e-02  5.36177978e-02\n",
      "   1.52161596e-02 -3.68847959e-02  3.29787098e-02  7.12669492e-02\n",
      "  -1.05379820e-01  3.51585895e-02 -1.08300075e-02 -1.27813285e-02\n",
      "   1.05999596e-02  7.56042078e-02  7.80663490e-02  1.00660495e-01\n",
      "  -8.26654211e-02 -4.56319600e-02  1.50279626e-02  2.26806011e-02\n",
      "   2.07112134e-02  5.10540232e-02 -5.25391288e-02 -3.79972272e-02\n",
      "  -4.27177064e-02 -1.92963183e-02 -1.79967145e-03 -4.53411788e-02\n",
      "   2.20028665e-02  4.29945737e-02 -1.89395268e-02  7.60603521e-04\n",
      "  -1.64834596e-03 -4.17581759e-02  1.84560679e-02 -8.06218982e-02\n",
      "  -8.27948079e-02  7.66004175e-02 -4.40564342e-02 -4.53449823e-02\n",
      "  -1.14374813e-02 -6.94235205e-04 -5.69598004e-02 -1.40871461e-02\n",
      "   4.47974280e-02  3.31295766e-02 -1.33966552e-02  5.01063354e-02\n",
      "  -4.92383055e-02 -5.70229292e-02  6.76584840e-02 -7.09209889e-02\n",
      "  -3.25187342e-04  8.48002434e-02  4.46085483e-02 -1.10244393e-01\n",
      "  -7.15760468e-03  6.81303293e-02 -9.70087424e-02  4.88211028e-02\n",
      "  -6.70844764e-02  8.89245700e-03  4.96959686e-02  2.65035182e-02\n",
      "  -7.95668289e-02 -6.50647804e-02  2.98870876e-02  3.71647403e-02\n",
      "  -1.56690814e-02  4.44842242e-02 -2.65724249e-02 -1.35066193e-02\n",
      "   2.59718951e-02 -2.87113916e-02  7.30632991e-02  5.64392991e-02\n",
      "  -5.46523668e-02 -6.48628501e-03 -2.07075831e-02 -3.58693376e-02\n",
      "  -4.72656116e-02  5.33407554e-02 -4.31216992e-02 -4.68572751e-02\n",
      "   5.37926033e-02  4.06456217e-02  1.96348671e-02  9.28576216e-02]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ram/Codes/college/chatbot/local-langchain/venv/lib/python3.8/site-packages/scipy/stats/_stats_py.py:4424: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(stats.ConstantInputWarning(msg))\n",
      "/home/ram/Codes/college/chatbot/local-langchain/venv/lib/python3.8/site-packages/scipy/stats/_stats_py.py:4916: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(stats.ConstantInputWarning(warn_msg))\n",
      "/home/ram/Codes/college/chatbot/local-langchain/venv/lib/python3.8/site-packages/sklearn/metrics/_ranking.py:980: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n",
      "/home/ram/Codes/college/chatbot/local-langchain/venv/lib/python3.8/site-packages/sklearn/metrics/_ranking.py:980: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n",
      "/home/ram/Codes/college/chatbot/local-langchain/venv/lib/python3.8/site-packages/sklearn/metrics/_ranking.py:980: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n",
      "/home/ram/Codes/college/chatbot/local-langchain/venv/lib/python3.8/site-packages/sklearn/metrics/_ranking.py:980: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial embedding evaluation results: {'pearson_cosine': nan, 'spearman_cosine': nan, 'pearson_manhattan': nan, 'spearman_manhattan': nan, 'pearson_euclidean': nan, 'spearman_euclidean': nan, 'pearson_dot': nan, 'spearman_dot': nan, 'pearson_max': nan, 'spearman_max': nan}\n",
      "Initial binary evaluation results: {'validation_cosine_accuracy': 0.9859154929577465, 'validation_cosine_accuracy_threshold': 0.9445478916168213, 'validation_cosine_f1': 0, 'validation_cosine_f1_threshold': 0, 'validation_cosine_precision': 0, 'validation_cosine_recall': 0, 'validation_cosine_ap': -0.0, 'validation_dot_accuracy': 0.9859154929577465, 'validation_dot_accuracy_threshold': 0.9445480108261108, 'validation_dot_f1': 0, 'validation_dot_f1_threshold': 0, 'validation_dot_precision': 0, 'validation_dot_recall': 0, 'validation_dot_ap': -0.0, 'validation_manhattan_accuracy': 0.9859154929577465, 'validation_manhattan_accuracy_threshold': 5.143091678619385, 'validation_manhattan_f1': 0, 'validation_manhattan_f1_threshold': 0, 'validation_manhattan_precision': 0, 'validation_manhattan_recall': 0, 'validation_manhattan_ap': -0.0, 'validation_euclidean_accuracy': 0.9859154929577465, 'validation_euclidean_accuracy_threshold': 0.33238500356674194, 'validation_euclidean_f1': 0, 'validation_euclidean_f1_threshold': 0, 'validation_euclidean_precision': 0, 'validation_euclidean_recall': 0, 'validation_euclidean_ap': -0.0, 'validation_max_accuracy': 0.9859154929577465, 'validation_max_accuracy_threshold': 5.143091678619385, 'validation_max_f1': 0, 'validation_max_f1_threshold': 0, 'validation_max_precision': 0, 'validation_max_recall': 0, 'validation_max_ap': -0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ram/Codes/college/chatbot/local-langchain/venv/lib/python3.8/site-packages/accelerate/accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n",
      " 48%|████▊     | 38/80 [00:10<00:10,  3.93it/s]"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 96.00 MiB. GPU ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 91\u001b[0m\n\u001b[1;32m     88\u001b[0m epochs_no_improve \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[0;32m---> 91\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_objectives\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loss\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     93\u001b[0m \u001b[43m        \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     94\u001b[0m \u001b[43m        \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwarmup_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwarmup_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     96\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     97\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptimizer_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1e-4\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Increase learning rate if necessary\u001b[39;49;00m\n\u001b[1;32m     98\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    100\u001b[0m     embedding_results \u001b[38;5;241m=\u001b[39m embedding_similarity_evaluator(model, output_path)\n\u001b[1;32m    101\u001b[0m     binary_results \u001b[38;5;241m=\u001b[39m binary_classification_evaluator(model, output_path)\n",
      "File \u001b[0;32m~/Codes/college/chatbot/local-langchain/venv/lib/python3.8/site-packages/sentence_transformers/fit_mixin.py:372\u001b[0m, in \u001b[0;36mFitMixin.fit\u001b[0;34m(self, train_objectives, evaluator, epochs, steps_per_epoch, scheduler, warmup_steps, optimizer_class, optimizer_params, weight_decay, evaluation_steps, output_path, save_best_model, max_grad_norm, use_amp, callback, show_progress_bar, checkpoint_path, checkpoint_save_steps, checkpoint_save_total_limit)\u001b[0m\n\u001b[1;32m    369\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    370\u001b[0m     trainer\u001b[38;5;241m.\u001b[39madd_callback(SaveModelCallback(output_path, evaluator, save_best_model))\n\u001b[0;32m--> 372\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Codes/college/chatbot/local-langchain/venv/lib/python3.8/site-packages/transformers/trainer.py:1780\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1778\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1779\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1780\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1781\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1782\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1783\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1784\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1785\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Codes/college/chatbot/local-langchain/venv/lib/python3.8/site-packages/transformers/trainer.py:2118\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2115\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_begin(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[1;32m   2117\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[0;32m-> 2118\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2120\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2121\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2122\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m   2123\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   2124\u001b[0m ):\n\u001b[1;32m   2125\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2126\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m~/Codes/college/chatbot/local-langchain/venv/lib/python3.8/site-packages/transformers/trainer.py:3036\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   3033\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb\u001b[38;5;241m.\u001b[39mreduce_mean()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m   3035\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[0;32m-> 3036\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3038\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mn_gpu \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   3039\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mmean()  \u001b[38;5;66;03m# mean() to average on multi-gpu parallel training\u001b[39;00m\n",
      "File \u001b[0;32m~/Codes/college/chatbot/local-langchain/venv/lib/python3.8/site-packages/sentence_transformers/trainer.py:329\u001b[0m, in \u001b[0;36mSentenceTransformerTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m    323\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    324\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mparallel_mode \u001b[38;5;241m!=\u001b[39m ParallelMode\u001b[38;5;241m.\u001b[39mNOT_PARALLEL\n\u001b[1;32m    325\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(model, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodule\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    326\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(loss_fn, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    327\u001b[0m ):\n\u001b[1;32m    328\u001b[0m     loss_fn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moverride_model_in_loss(loss_fn, model)\n\u001b[0;32m--> 329\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mloss_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    330\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_outputs:\n\u001b[1;32m    331\u001b[0m     \u001b[38;5;66;03m# During prediction/evaluation, `compute_loss` will be called with `return_outputs=True`.\u001b[39;00m\n\u001b[1;32m    332\u001b[0m     \u001b[38;5;66;03m# However, Sentence Transformer losses do not return outputs, so we return an empty dictionary.\u001b[39;00m\n\u001b[1;32m    333\u001b[0m     \u001b[38;5;66;03m# This does not result in any problems, as the SentenceTransformerTrainingArguments sets\u001b[39;00m\n\u001b[1;32m    334\u001b[0m     \u001b[38;5;66;03m# `prediction_loss_only=True` which means that the output is not used.\u001b[39;00m\n\u001b[1;32m    335\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss, {}\n",
      "File \u001b[0;32m~/Codes/college/chatbot/local-langchain/venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Codes/college/chatbot/local-langchain/venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Codes/college/chatbot/local-langchain/venv/lib/python3.8/site-packages/sentence_transformers/losses/MultipleNegativesRankingLoss.py:93\u001b[0m, in \u001b[0;36mMultipleNegativesRankingLoss.forward\u001b[0;34m(self, sentence_features, labels)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, sentence_features: Iterable[Dict[\u001b[38;5;28mstr\u001b[39m, Tensor]], labels: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m---> 93\u001b[0m     reps \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(sentence_feature)[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msentence_embedding\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m sentence_feature \u001b[38;5;129;01min\u001b[39;00m sentence_features]\n\u001b[1;32m     94\u001b[0m     embeddings_a \u001b[38;5;241m=\u001b[39m reps[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     95\u001b[0m     embeddings_b \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(reps[\u001b[38;5;241m1\u001b[39m:])\n",
      "File \u001b[0;32m~/Codes/college/chatbot/local-langchain/venv/lib/python3.8/site-packages/sentence_transformers/losses/MultipleNegativesRankingLoss.py:93\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, sentence_features: Iterable[Dict[\u001b[38;5;28mstr\u001b[39m, Tensor]], labels: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m---> 93\u001b[0m     reps \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentence_feature\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msentence_embedding\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m sentence_feature \u001b[38;5;129;01min\u001b[39;00m sentence_features]\n\u001b[1;32m     94\u001b[0m     embeddings_a \u001b[38;5;241m=\u001b[39m reps[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     95\u001b[0m     embeddings_b \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(reps[\u001b[38;5;241m1\u001b[39m:])\n",
      "File \u001b[0;32m~/Codes/college/chatbot/local-langchain/venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Codes/college/chatbot/local-langchain/venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Codes/college/chatbot/local-langchain/venv/lib/python3.8/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/Codes/college/chatbot/local-langchain/venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Codes/college/chatbot/local-langchain/venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Codes/college/chatbot/local-langchain/venv/lib/python3.8/site-packages/sentence_transformers/models/Transformer.py:118\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[0;34m(self, features)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken_type_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m features:\n\u001b[1;32m    116\u001b[0m     trans_features[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken_type_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m features[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken_type_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m--> 118\u001b[0m output_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mauto_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtrans_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    119\u001b[0m output_tokens \u001b[38;5;241m=\u001b[39m output_states[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    121\u001b[0m features\u001b[38;5;241m.\u001b[39mupdate({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken_embeddings\u001b[39m\u001b[38;5;124m\"\u001b[39m: output_tokens, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m: features[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m]})\n",
      "File \u001b[0;32m~/Codes/college/chatbot/local-langchain/venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Codes/college/chatbot/local-langchain/venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Codes/college/chatbot/local-langchain/venv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py:1013\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1004\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m   1006\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings(\n\u001b[1;32m   1007\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   1008\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1011\u001b[0m     past_key_values_length\u001b[38;5;241m=\u001b[39mpast_key_values_length,\n\u001b[1;32m   1012\u001b[0m )\n\u001b[0;32m-> 1013\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1014\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1015\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1016\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1017\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1018\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1019\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1020\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1021\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1022\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1023\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1024\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1025\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1026\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Codes/college/chatbot/local-langchain/venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Codes/college/chatbot/local-langchain/venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Codes/college/chatbot/local-langchain/venv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py:607\u001b[0m, in \u001b[0;36mBertEncoder.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    596\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    597\u001b[0m         layer_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    598\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    604\u001b[0m         output_attentions,\n\u001b[1;32m    605\u001b[0m     )\n\u001b[1;32m    606\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 607\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    608\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    609\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    610\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    611\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    612\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    613\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    614\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    615\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    617\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    618\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/Codes/college/chatbot/local-langchain/venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Codes/college/chatbot/local-langchain/venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Codes/college/chatbot/local-langchain/venv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py:497\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    485\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    486\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    487\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    494\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[1;32m    495\u001b[0m     \u001b[38;5;66;03m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[39;00m\n\u001b[1;32m    496\u001b[0m     self_attn_past_key_value \u001b[38;5;241m=\u001b[39m past_key_value[:\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 497\u001b[0m     self_attention_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    499\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mself_attn_past_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    503\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    504\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m self_attention_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    506\u001b[0m     \u001b[38;5;66;03m# if decoder, the last output is tuple of self-attn cache\u001b[39;00m\n",
      "File \u001b[0;32m~/Codes/college/chatbot/local-langchain/venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Codes/college/chatbot/local-langchain/venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Codes/college/chatbot/local-langchain/venv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py:427\u001b[0m, in \u001b[0;36mBertAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    417\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    418\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    419\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    425\u001b[0m     output_attentions: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    426\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[0;32m--> 427\u001b[0m     self_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    428\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    429\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    430\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    431\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    432\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    433\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    434\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    435\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    436\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput(self_outputs[\u001b[38;5;241m0\u001b[39m], hidden_states)\n\u001b[1;32m    437\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (attention_output,) \u001b[38;5;241m+\u001b[39m self_outputs[\u001b[38;5;241m1\u001b[39m:]  \u001b[38;5;66;03m# add attentions if we output them\u001b[39;00m\n",
      "File \u001b[0;32m~/Codes/college/chatbot/local-langchain/venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Codes/college/chatbot/local-langchain/venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Codes/college/chatbot/local-langchain/venv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py:349\u001b[0m, in \u001b[0;36mBertSelfAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    346\u001b[0m         relative_position_scores_key \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39meinsum(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbhrd,lrd->bhlr\u001b[39m\u001b[38;5;124m\"\u001b[39m, key_layer, positional_embedding)\n\u001b[1;32m    347\u001b[0m         attention_scores \u001b[38;5;241m=\u001b[39m attention_scores \u001b[38;5;241m+\u001b[39m relative_position_scores_query \u001b[38;5;241m+\u001b[39m relative_position_scores_key\n\u001b[0;32m--> 349\u001b[0m attention_scores \u001b[38;5;241m=\u001b[39m \u001b[43mattention_scores\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqrt\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention_head_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    351\u001b[0m     \u001b[38;5;66;03m# Apply the attention mask is (precomputed for all layers in BertModel forward() function)\u001b[39;00m\n\u001b[1;32m    352\u001b[0m     attention_scores \u001b[38;5;241m=\u001b[39m attention_scores \u001b[38;5;241m+\u001b[39m attention_mask\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 96.00 MiB. GPU "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader\n",
    "from sentence_transformers import SentenceTransformer, InputExample, losses\n",
    "from sentence_transformers.evaluation import EmbeddingSimilarityEvaluator, BinaryClassificationEvaluator\n",
    "import math\n",
    "import torch\n",
    "\n",
    "from models import models, ModelName\n",
    "\n",
    "# Load and inspect the dataset\n",
    "csv_file = 'fine_tuning_dataset/all_merged_dataset.csv'\n",
    "df = pd.read_csv(csv_file)\n",
    "\n",
    "# Check for missing values\n",
    "print(\"Missing values in dataset:\\n\", df.isnull().sum())\n",
    "\n",
    "# Drop rows with any missing values\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# Ensure there are no empty strings\n",
    "df['question'] = df['question'].apply(lambda x: x.strip() if isinstance(x, str) else x)\n",
    "df['context'] = df['context'].apply(lambda x: x.strip() if isinstance(x, str) else x)\n",
    "\n",
    "# Verify dataset after cleaning\n",
    "print(\"Dataset after cleaning:\\n\", df.head())\n",
    "\n",
    "# Split the dataset into training and validation sets\n",
    "train_df, val_df = train_test_split(df, test_size=0.1, random_state=42)\n",
    "\n",
    "# Ensure the validation dataset has pairs and valid data\n",
    "print(f\"Number of validation examples: {len(val_df)}\")\n",
    "print(f\"Sample validation data: {val_df.head()}\")\n",
    "\n",
    "# Manually inspect some examples for a sanity check\n",
    "for i in range(5):\n",
    "    print(f\"Example {i+1}: Question - {val_df.iloc[i]['question']}, Context - {val_df.iloc[i]['context']}\")\n",
    "\n",
    "# Create InputExamples\n",
    "train_examples = [InputExample(texts=[row['question'], row['context']]) for _, row in train_df.iterrows()]\n",
    "val_examples = [InputExample(texts=[row['question'], row['context']]) for _, row in val_df.iterrows()]\n",
    "\n",
    "print(f\"First few training examples: {train_examples[:5]}\")\n",
    "print(f\"First few validation examples: {val_examples[:5]}\")\n",
    "\n",
    "# Create DataLoaders\n",
    "train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=8)\n",
    "val_dataloader = DataLoader(val_examples, shuffle=False, batch_size=8)\n",
    "\n",
    "# Load the pre-trained model\n",
    "# model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')\n",
    "model = SentenceTransformer('intfloat/multilingual-e5-small')\n",
    "\n",
    "# Test embedding generation\n",
    "sample_texts = [train_examples[0].texts[0], train_examples[0].texts[1]]\n",
    "embeddings = model.encode(sample_texts)\n",
    "print(\"Sample embeddings:\", embeddings)\n",
    "\n",
    "# Use MultipleNegativesRankingLoss for training\n",
    "train_loss = losses.MultipleNegativesRankingLoss(model)\n",
    "\n",
    "# Use EmbeddingSimilarityEvaluator for validation\n",
    "embedding_similarity_evaluator = EmbeddingSimilarityEvaluator.from_input_examples(val_examples, batch_size=8)\n",
    "\n",
    "# Use BinaryClassificationEvaluator for a simpler evaluation test\n",
    "binary_classification_evaluator = BinaryClassificationEvaluator.from_input_examples(val_examples, batch_size=8, name='validation')\n",
    "\n",
    "# Initial validation to check for proper evaluator functioning\n",
    "embedding_initial_results = embedding_similarity_evaluator(model)\n",
    "binary_initial_results = binary_classification_evaluator(model)\n",
    "print(f\"Initial embedding evaluation results: {embedding_initial_results}\")\n",
    "print(f\"Initial binary evaluation results: {binary_initial_results}\")\n",
    "\n",
    "# Set device\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device = torch.device('cpu')\n",
    "model.to(device)\n",
    "\n",
    "# Fine-tune the model with early stopping\n",
    "num_epochs = 50\n",
    "warmup_steps = math.ceil(len(train_dataloader) * num_epochs * 0.1)\n",
    "model_name = ModelName.MULTILINGUAL_MINILM_FINETUNING_EARLY_STOP.value\n",
    "output_path = models[model_name]['local_dir']\n",
    "\n",
    "# Early stopping parameters\n",
    "patience = 5\n",
    "best_score = float('-inf')\n",
    "epochs_no_improve = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.fit(\n",
    "        train_objectives=[(train_dataloader, train_loss)],\n",
    "        epochs=1,\n",
    "        steps_per_epoch=len(train_dataloader),\n",
    "        warmup_steps=warmup_steps,\n",
    "        output_path=output_path,\n",
    "        optimizer_params={'lr': 1e-4}  # Increase learning rate if necessary\n",
    "    )\n",
    "\n",
    "    embedding_results = embedding_similarity_evaluator(model, output_path)\n",
    "    binary_results = binary_classification_evaluator(model, output_path)\n",
    "    print(f\"Epoch {epoch+1} embedding evaluation results: {embedding_results}\")\n",
    "    print(f\"Epoch {epoch+1} binary evaluation results: {binary_results}\")\n",
    "\n",
    "    # Use validation_manhattan_accuracy as the metric for early stopping\n",
    "    score = binary_results.get('validation_manhattan_accuracy', float('-inf'))\n",
    "    \n",
    "    print(f\"=====> embedding score {embedding_results}, binary score {score}, best score {best_score}\")\n",
    "\n",
    "    if score > best_score:\n",
    "        best_score = score\n",
    "        epochs_no_improve = 0\n",
    "        model.save(output_path)  # Save the best model\n",
    "        print(f\"New best score: {best_score}. Model saved.\")\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "        print(f\"No improvement. {epochs_no_improve}/{patience} patience periods passed.\")\n",
    "    \n",
    "    if epochs_no_improve >= patience:\n",
    "        print(f\"Early stopping triggered after {epoch + 1} epochs.\")\n",
    "        break\n",
    "\n",
    "print(f\"Model fine-tuning complete. Model saved to {output_path}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example evaluation - you may need to adapt according to your dataset\n",
    "evaluation_examples = [\n",
    "    (\"How are you?\", \"How do you do?\"),\n",
    "    (\"What is your name?\", \"What's your name?\"),\n",
    "    (\"Where do you live?\", \"Where is your home located?\")\n",
    "]\n",
    "\n",
    "model = SentenceTransformer(output_path)\n",
    "\n",
    "for pair in evaluation_examples:\n",
    "    embeddings = model.encode(pair)\n",
    "    similarity = util.pytorch_cos_sim(embeddings[0], embeddings[1])\n",
    "    print(f\"Similarity between: '{pair[0]}' and '{pair[1]}' is {similarity.item():.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
