{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.evaluation import SemanticSimilarityEvaluator\n",
    "\n",
    "evaluator = SemanticSimilarityEvaluator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This evaluator only uses `response` and `reference`, passing in query does not influence the evaluation\n",
    "# query = 'What is the color of the sky'\n",
    "\n",
    "response = \"The sky is typically blue\"\n",
    "reference = \"\"\"The color of the sky can vary depending on several factors, including time of day, weather conditions, and location.\n",
    "\n",
    "During the day, when the sun is in the sky, the sky often appears blue. \n",
    "This is because of a phenomenon called Rayleigh scattering, where molecules and particles in the Earth's atmosphere scatter sunlight in all directions, and blue light is scattered more than other colors because it travels as shorter, smaller waves. \n",
    "This is why we perceive the sky as blue on a clear day.\n",
    "\"\"\"\n",
    "\n",
    "result = await evaluator.aevaluate(\n",
    "    response=response,\n",
    "    reference=reference,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score:  0.8741614884630504\n",
      "Passing:  True\n"
     ]
    }
   ],
   "source": [
    "print(\"Score: \", result.score)\n",
    "print(\"Passing: \", result.passing)  # default similarity threshold is 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = \"Sorry, I do not have sufficient context to answer this question.\"\n",
    "reference = \"\"\"The color of the sky can vary depending on several factors, including time of day, weather conditions, and location.\n",
    "\n",
    "During the day, when the sun is in the sky, the sky often appears blue. \n",
    "This is because of a phenomenon called Rayleigh scattering, where molecules and particles in the Earth's atmosphere scatter sunlight in all directions, and blue light is scattered more than other colors because it travels as shorter, smaller waves. \n",
    "This is why we perceive the sky as blue on a clear day.\n",
    "\"\"\"\n",
    "\n",
    "result = await evaluator.aevaluate(\n",
    "    response=response,\n",
    "    reference=reference,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score:  0.7213441101430748\n",
      "Passing:  False\n"
     ]
    }
   ],
   "source": [
    "print(\"Score: \", result.score)\n",
    "print(\"Passing: \", result.passing)  # default similarity threshold is 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarity Matrix:\n",
      "tensor([[1.0000, 0.7915, 0.9290],\n",
      "        [0.7915, 1.0000, 0.7434],\n",
      "        [0.9290, 0.7434, 1.0000]])\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "import numpy as np\n",
    "\n",
    "# Load the embedding model from Hugging Face\n",
    "model_name = 'sentence-transformers/all-MiniLM-L6-v2'\n",
    "model = SentenceTransformer(model_name)\n",
    "\n",
    "# Sample data\n",
    "sentences = [\n",
    "    \"The quick brown fox jumps over the lazy dog.\",\n",
    "    \"A fast, dark-colored fox leaps over a sleepy canine.\",\n",
    "    \"The lazy dog was jumped over by a quick brown fox.\"\n",
    "]\n",
    "\n",
    "# Compute embeddings\n",
    "embeddings = model.encode(sentences)\n",
    "\n",
    "# Calculate cosine similarity matrix\n",
    "cosine_sim_matrix = util.cos_sim(embeddings, embeddings)\n",
    "\n",
    "print(\"Cosine Similarity Matrix:\")\n",
    "print(cosine_sim_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ram/Codes/college/chatbot/local-langchain/venv/lib/python3.8/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silhouette Score: 0.2969221770763397\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# Clustering with K-Means\n",
    "num_clusters = 2\n",
    "kmeans = KMeans(n_clusters=num_clusters, random_state=0)\n",
    "cluster_labels = kmeans.fit_predict(embeddings)\n",
    "\n",
    "# Evaluate clustering using silhouette score\n",
    "silhouette_avg = silhouette_score(embeddings, cluster_labels)\n",
    "print(f\"Silhouette Score: {silhouette_avg}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarity between Question and Context:\n",
      "tensor([[0.6925]])\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "# Load the embedding model\n",
    "model_name = './app/model/modules/multilingual-minilm'\n",
    "model = SentenceTransformer(model_name)\n",
    "\n",
    "# Sample question and context\n",
    "question = \"What is the capital of France?\"\n",
    "context = (\n",
    "    \"Paris, the capital of France, is known for its art, fashion, and culture. \"\n",
    "    \"It is one of the most popular tourist destinations in the world.\"\n",
    ")\n",
    "\n",
    "# Compute embeddings\n",
    "question_embedding = model.encode(question)\n",
    "context_embedding = model.encode(context)\n",
    "\n",
    "# Calculate cosine similarity\n",
    "cosine_similarity = util.cos_sim(question_embedding, context_embedding)\n",
    "\n",
    "print(\"Cosine Similarity between Question and Context:\")\n",
    "print(cosine_similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score:  ./app/model/modules/multilingual-minilm-finetuning-6 0.6160385790316049\n",
      "Passing:  True\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.evaluation import SemanticSimilarityEvaluator\n",
    "from llama_index.core.embeddings import resolve_embed_model\n",
    "from models import models, ModelName\n",
    "model_name = models[ModelName.MULTILINGUAL_MINILM_FINETUNING_6.value][\"local_dir\"]\n",
    "\n",
    "embed_model = resolve_embed_model(f\"local:{model_name}\")\n",
    "evaluator = SemanticSimilarityEvaluator(\n",
    "    embed_model=embed_model,\n",
    "    similarity_threshold=0.6,\n",
    ")\n",
    "response = \"The sky is yellow.\"\n",
    "reference = \"The sky is blue.\"\n",
    "\n",
    "result = await evaluator.aevaluate(\n",
    "    response=response,\n",
    "    reference=reference,\n",
    ")\n",
    "print(\"Score: \", model_name, result.score)\n",
    "print(\"Passing: \", result.passing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: ./app/model/modules/indo-sentence-bert-base\n",
      "Cosine Similarity Matrix for Combined Embeddings:\n",
      "[[1.         0.27892458 0.13558199]\n",
      " [0.27892458 0.99999994 0.17862284]\n",
      " [0.13558197 0.17862283 0.99999994]]\n",
      "\n",
      "Average Similarity Score (excluding diagonal):\n",
      "0.1977098\n",
      "\n",
      "==================================================\n",
      "\n",
      "Model: ./app/model/modules/all-minilm\n",
      "Cosine Similarity Matrix for Combined Embeddings:\n",
      "[[0.9999999  0.33218047 0.2400226 ]\n",
      " [0.33218047 1.         0.31461537]\n",
      " [0.24002257 0.31461537 1.        ]]\n",
      "\n",
      "Average Similarity Score (excluding diagonal):\n",
      "0.29560614\n",
      "\n",
      "==================================================\n",
      "\n",
      "Model: ./app/model/modules/mpnet-base-v2\n",
      "Cosine Similarity Matrix for Combined Embeddings:\n",
      "[[1.0000001  0.33152586 0.2571634 ]\n",
      " [0.33152586 1.         0.26290733]\n",
      " [0.25716344 0.26290733 1.0000001 ]]\n",
      "\n",
      "Average Similarity Score (excluding diagonal):\n",
      "0.28386554\n",
      "\n",
      "==================================================\n",
      "\n",
      "Model: ./app/model/modules/multilingual-minilm\n",
      "Cosine Similarity Matrix for Combined Embeddings:\n",
      "[[1.         0.28806776 0.21676947]\n",
      " [0.28806776 1.         0.29232207]\n",
      " [0.21676949 0.29232204 1.        ]]\n",
      "\n",
      "Average Similarity Score (excluding diagonal):\n",
      "0.26571977\n",
      "\n",
      "==================================================\n",
      "\n",
      "Model: ./app/model/modules/labse\n",
      "Cosine Similarity Matrix for Combined Embeddings:\n",
      "[[1.0000001  0.26739228 0.24765497]\n",
      " [0.26739228 1.0000001  0.24439073]\n",
      " [0.24765497 0.24439073 1.        ]]\n",
      "\n",
      "Average Similarity Score (excluding diagonal):\n",
      "0.253146\n",
      "\n",
      "==================================================\n",
      "\n",
      "Model: ./app/model/modules/multilingual-e5-small\n",
      "Cosine Similarity Matrix for Combined Embeddings:\n",
      "[[0.9999999  0.63242215 0.59577376]\n",
      " [0.63242215 1.         0.6100215 ]\n",
      " [0.5957738  0.6100215  1.        ]]\n",
      "\n",
      "Average Similarity Score (excluding diagonal):\n",
      "0.61273915\n",
      "\n",
      "==================================================\n",
      "\n",
      "Model: ./app/model/modules/multilingual-minilm-finetuning\n",
      "Cosine Similarity Matrix for Combined Embeddings:\n",
      "[[0.99999994 0.30329502 0.23670205]\n",
      " [0.30329502 1.         0.30435848]\n",
      " [0.23670205 0.3043585  1.        ]]\n",
      "\n",
      "Average Similarity Score (excluding diagonal):\n",
      "0.28145185\n",
      "\n",
      "==================================================\n",
      "\n",
      "Model: ./app/model/modules/multilingual-minilm-finetuning-2\n",
      "Cosine Similarity Matrix for Combined Embeddings:\n",
      "[[1.0000001  0.31106487 0.23542738]\n",
      " [0.31106487 0.9999999  0.30021632]\n",
      " [0.2354274  0.30021632 1.        ]]\n",
      "\n",
      "Average Similarity Score (excluding diagonal):\n",
      "0.2822362\n",
      "\n",
      "==================================================\n",
      "\n",
      "Model: ./app/model/modules/multilingual-minilm-finetuning-3\n",
      "Cosine Similarity Matrix for Combined Embeddings:\n",
      "[[1.         0.29377088 0.25054428]\n",
      " [0.29377088 0.9999999  0.30514482]\n",
      " [0.25054425 0.3051448  1.        ]]\n",
      "\n",
      "Average Similarity Score (excluding diagonal):\n",
      "0.28315333\n",
      "\n",
      "==================================================\n",
      "\n",
      "Model: ./app/model/modules/multilingual-minilm-finetuning-4\n",
      "Cosine Similarity Matrix for Combined Embeddings:\n",
      "[[0.99999994 0.31037435 0.2655087 ]\n",
      " [0.31037435 1.         0.3119912 ]\n",
      " [0.2655087  0.3119912  1.        ]]\n",
      "\n",
      "Average Similarity Score (excluding diagonal):\n",
      "0.29595807\n",
      "\n",
      "==================================================\n",
      "\n",
      "Model: ./app/model/modules/multilingual-minilm-finetuning-5\n",
      "Cosine Similarity Matrix for Combined Embeddings:\n",
      "[[1.0000001  0.28679928 0.26705226]\n",
      " [0.28679928 1.0000001  0.31114918]\n",
      " [0.26705226 0.31114918 0.9999998 ]]\n",
      "\n",
      "Average Similarity Score (excluding diagonal):\n",
      "0.28833356\n",
      "\n",
      "==================================================\n",
      "\n",
      "Model: ./app/model/modules/multilingual-minilm-finetuning-6\n",
      "Cosine Similarity Matrix for Combined Embeddings:\n",
      "[[0.99999994 0.29429483 0.26933253]\n",
      " [0.29429483 0.99999976 0.3110533 ]\n",
      " [0.26933253 0.3110533  0.99999994]]\n",
      "\n",
      "Average Similarity Score (excluding diagonal):\n",
      "0.2915602\n",
      "\n",
      "==================================================\n",
      "\n",
      "Model: ./app/model/modules/multilingual-e5-small-finetuning-1\n",
      "Cosine Similarity Matrix for Combined Embeddings:\n",
      "[[1.         0.31169963 0.2209515 ]\n",
      " [0.31169963 0.99999994 0.30032396]\n",
      " [0.2209515  0.30032396 1.0000002 ]]\n",
      "\n",
      "Average Similarity Score (excluding diagonal):\n",
      "0.27765837\n",
      "\n",
      "==================================================\n",
      "\n",
      "Model: ./app/model/modules/multilingual-e5-small-finetuning-2\n",
      "Cosine Similarity Matrix for Combined Embeddings:\n",
      "[[0.99999994 0.30557585 0.2450939 ]\n",
      " [0.30557585 0.9999999  0.29952186]\n",
      " [0.24509388 0.2995219  1.        ]]\n",
      "\n",
      "Average Similarity Score (excluding diagonal):\n",
      "0.2833972\n",
      "\n",
      "==================================================\n",
      "\n",
      "Model: ./app/model/modules/multilingual-e5-small-finetuning-3\n",
      "Cosine Similarity Matrix for Combined Embeddings:\n",
      "[[0.9999998  0.29745132 0.2579692 ]\n",
      " [0.29745132 1.         0.30814263]\n",
      " [0.2579692  0.30814263 1.        ]]\n",
      "\n",
      "Average Similarity Score (excluding diagonal):\n",
      "0.2878544\n",
      "\n",
      "==================================================\n",
      "\n",
      "Model: ./app/model/modules/multilingual-e5-small-finetuning-4\n",
      "Cosine Similarity Matrix for Combined Embeddings:\n",
      "[[1.         0.29261982 0.26029152]\n",
      " [0.29261982 1.0000001  0.31744713]\n",
      " [0.26029152 0.31744713 0.9999999 ]]\n",
      "\n",
      "Average Similarity Score (excluding diagonal):\n",
      "0.2901195\n",
      "\n",
      "==================================================\n",
      "\n",
      "Model: ./app/model/modules/multilingual-e5-small-finetuning-5\n",
      "Cosine Similarity Matrix for Combined Embeddings:\n",
      "[[1.0000001  0.2971486  0.25983977]\n",
      " [0.2971486  0.99999994 0.30895603]\n",
      " [0.25983977 0.30895603 0.9999998 ]]\n",
      "\n",
      "Average Similarity Score (excluding diagonal):\n",
      "0.28864813\n",
      "\n",
      "==================================================\n",
      "\n",
      "Model: ./app/model/modules/multilingual-e5-small-finetuning-6\n",
      "Cosine Similarity Matrix for Combined Embeddings:\n",
      "[[1.0000001  0.28719056 0.27873847]\n",
      " [0.28719056 1.         0.3214925 ]\n",
      " [0.27873844 0.32149252 1.0000001 ]]\n",
      "\n",
      "Average Similarity Score (excluding diagonal):\n",
      "0.29580718\n",
      "\n",
      "==================================================\n",
      "\n",
      "Model: ./app/model/modules/multilingual-minilm-earlystop\n",
      "Cosine Similarity Matrix for Combined Embeddings:\n",
      "[[1.         0.28834635 0.21678439]\n",
      " [0.28834635 0.9999999  0.29239607]\n",
      " [0.21678439 0.29239607 1.0000001 ]]\n",
      "\n",
      "Average Similarity Score (excluding diagonal):\n",
      "0.2658423\n",
      "\n",
      "==================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "import torch\n",
    "import numpy as np\n",
    "from models import models, ModelName\n",
    "\n",
    "for i in models:\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.ipc_collect()\n",
    "    # Load the embedding model\n",
    "    model_name = models[i][\"local_dir\"]\n",
    "    model = SentenceTransformer(model_name, device=\"cpu\")\n",
    "\n",
    "    # Sample questions and contexts\n",
    "    questions = [\n",
    "        \"What is the capital of France?\",\n",
    "        \"How many continents are there?\",\n",
    "        \"Who wrote '1984'?\"\n",
    "    ]\n",
    "\n",
    "    contexts = [\n",
    "        \"Paris, the capital of France, is known for its art, fashion, and culture. It is one of the most popular tourist destinations in the world.\",\n",
    "        \"There are seven continents on Earth: Africa, Antarctica, Asia, Europe, North America, Australia, and South America.\",\n",
    "        \"'1984' is a dystopian social science fiction novel and cautionary tale, written by the English writer George Orwell.\"\n",
    "    ]\n",
    "\n",
    "    # Ensure the lengths of questions and contexts are the same\n",
    "    assert len(questions) == len(contexts), \"The number of questions must match the number of contexts\"\n",
    "\n",
    "    # Compute embeddings for all questions and contexts\n",
    "    with torch.no_grad():\n",
    "        question_embeddings = model.encode(questions, convert_to_tensor=True)\n",
    "        context_embeddings = model.encode(contexts, convert_to_tensor=True)\n",
    "\n",
    "        # Perform element-wise multiplication for each question-context pair\n",
    "        combined_embeddings = torch.mul(question_embeddings, context_embeddings)\n",
    "\n",
    "        # Calculate cosine similarity matrix for the combined embeddings\n",
    "        similarity_matrix = util.cos_sim(combined_embeddings, combined_embeddings).cpu().numpy()\n",
    "\n",
    "    # Extract the upper triangular part of the similarity matrix, excluding the diagonal\n",
    "    triu_indices = np.triu_indices_from(similarity_matrix, k=1)\n",
    "    triu_values = similarity_matrix[triu_indices]\n",
    "\n",
    "    # Calculate the average similarity score\n",
    "    average_similarity_score = np.mean(triu_values)\n",
    "\n",
    "    print(f\"Model: {model_name}\")\n",
    "    print(\"Cosine Similarity Matrix for Combined Embeddings:\")\n",
    "    print(similarity_matrix)\n",
    "    print(\"\\nAverage Similarity Score (excluding diagonal):\")\n",
    "    print(average_similarity_score)\n",
    "    print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.ipc_collect()\n",
    "    del model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
