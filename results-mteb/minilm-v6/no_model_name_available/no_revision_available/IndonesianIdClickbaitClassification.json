{
  "dataset_revision": "9fa4d0824015fe537ae2c8166781f5c79873da2c",
  "evaluation_time": 2.852938413619995,
  "kg_co2_emissions": null,
  "mteb_version": "1.12.23",
  "scores": {
    "train": [
      {
        "accuracy": 0.544287109375,
        "ap": 0.44629282091986183,
        "f1": 0.539208124093365,
        "hf_subset": "default",
        "languages": [
          "ind-Latn"
        ],
        "main_score": 0.539208124093365,
        "scores_per_experiment": [
          {
            "accuracy": 0.58203125,
            "ap": 0.46326581547941426,
            "f1": 0.5742997789286783
          },
          {
            "accuracy": 0.52685546875,
            "ap": 0.4432514466723309,
            "f1": 0.5265618771177252
          },
          {
            "accuracy": 0.498046875,
            "ap": 0.42821689646031674,
            "f1": 0.49735467925724747
          },
          {
            "accuracy": 0.57373046875,
            "ap": 0.46409080305211653,
            "f1": 0.571814390251788
          },
          {
            "accuracy": 0.5615234375,
            "ap": 0.4456645781872489,
            "f1": 0.5475429453165593
          },
          {
            "accuracy": 0.47509765625,
            "ap": 0.41730671574260825,
            "f1": 0.4739175056960808
          },
          {
            "accuracy": 0.54833984375,
            "ap": 0.4458410409127145,
            "f1": 0.544940817564362
          },
          {
            "accuracy": 0.5546875,
            "ap": 0.4591740658924269,
            "f1": 0.554615716987773
          },
          {
            "accuracy": 0.57080078125,
            "ap": 0.4570676006078461,
            "f1": 0.5645692997497267
          },
          {
            "accuracy": 0.5517578125,
            "ap": 0.4390492461915951,
            "f1": 0.5364642300637094
          }
        ]
      }
    ]
  },
  "task_name": "IndonesianIdClickbaitClassification"
}